{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import nd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input, num_hidden, num_output = 784, 256, 10 # the unit number of input, hidden and output layer\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_input, num_hidden)) # W1 is in R784*256\n",
    "b1 = nd.zeros(num_hidden)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hidden, num_output)) # W2 is in R256*10\n",
    "b2 = nd.zeros(num_output)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "# attach gradient\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Activation Function\n",
    "$$ReLU(x) = \\max(x, 0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return nd.maximum(x, 0) # x.relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "$$O = HW_2 + b_2 = \\phi(XW_1 + b_1)W_2 + b_1$$\n",
    "Here activation function $\\phi$ is **ReLU**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_input))\n",
    "    H = relu(nd.dot(X, W1) + b1) # \\phi\n",
    "    return nd.dot(H, W2) + b2 # HW_2 + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8454, train acc 0.686, test acc 0.828\n",
      "epoch 2, loss 0.4894, train acc 0.820, test acc 0.841\n",
      "epoch 3, loss 0.4354, train acc 0.838, test acc 0.854\n",
      "epoch 4, loss 0.3983, train acc 0.851, test acc 0.860\n",
      "epoch 5, loss 0.3799, train acc 0.859, test acc 0.872\n"
     ]
    }
   ],
   "source": [
    "num_epoch, lr = 5, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epoch, batch_size, params, lr)\n",
    "# epoch, loss 0.3799, train acc 0.859, test acc 0.872 \n",
    "# It seems that multilayer perceptron is better, because we add hidden layer and activation function\n",
    "# Before the test accuracy is 0.84 or 0.85 at most!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
