{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归的基本元素\n",
    "简单的房屋模型作为列子来解释线性回归的基本元素。   \n",
    "通过房屋的面积和房龄来预测房屋的价格（当然不可能值取决于这两个因素），接下来我们希望探索价格与这两个因素之间的关系。\n",
    "\n",
    "#### 模型\n",
    "设房屋的面积为$x_1$，房龄为$x_2$，售出的价格为$y$。线性回归假设这些变量满足上面的关系。\n",
    "$$\\hat y = x_1w_1 + x_2w_2 + b$$\n",
    "其中$w_1$和$w_2$是权重，$b$是误差，这些值被称为线性回归的**参数**，他们都是**标量**。我们一般允许公式存在一定的误差，毕竟给定的数据也不一定是百分之百线性分布的。\n",
    "\n",
    "#### 模型训练\n",
    "我们需要做的是通过数据寻找到特定的线性回归参数，使得误差最小，这个过程被称为**模型训练**。\n",
    "\n",
    "#### 训练数据\n",
    "假设我们采集的样本数为$n$，索引为$i$的样本的特征分别为$x_1^{\\left(i\\right)}$和$x_2^{\\left(i\\right)}$，标签为$y^{\\left(i\\right)}$。对于标签为$i$的房屋，通过线性回归预测的价格为\n",
    "$$\\hat y^{\\left(i\\right)} = x_1^{\\left(i\\right)}w_1 + x_2^{\\left(i\\right)}w_2 + b$$\n",
    "\n",
    "#### 损失函数\n",
    "在模型训练中，我们需要衡量价格预测和真实值之间的误差，通常的情况下我们选取一个非负数作为误差，且数值越小就代表误差越小。一个比较常用的选择是平方误差函数，评估索引为$i$的样本误差的表达式为\n",
    "$${\\scr l}^{(i)} = \\frac 1 2 (\\hat y^{(i)} - \\hat y^{(i)})^2 = \\frac12 (x_1^{(i)}w_1 + x_2^{(i)}w_2 + b - y^{(i)})^2$$\n",
    "其实$\\frac 1 2$是为了是后面求导之后没有分数。显然误差值越小就代表了预测的价格越准确，当误差为0的时候表示预测的值和真实的值是相等的。在机器学习中，将这种衡量误差的函数称为**损失函数**，上面的那个平方误差函数又被称为**平方损失**。  \n",
    "通常我们使用训练数据集中所有样本的的样本误差的平均值来衡量模型预测的质量，即：\n",
    "$${\\scr l}(w_1,\\ w_2,\\ b) = \\frac 1 n \\sum_{i=1}^n {\\scr l}^{(i)}(w_1,\\ w_2,\\ b) = \\frac 1 n \\sum_{i=1}^n \\frac 1 2 (x_1^{(i)}w_1 + x_2^{(i)}w_2 + b - y^{(i)})^2$$\n",
    "在模型训练中，我们需要找到一组模型参数$w_1^*$，$w_2^*$和$b^*$，使得训练样本的平均损失最小，即：\n",
    "$$w_1^*,\\ w_2^*,\\ b^* = \\mathop{\\arg\\min}\\limits_{w_1,\\ w_2,\\ b} {\\scr l}(w_1,\\ w_2,\\ b)$$\n",
    "\n",
    "#### 优化算法\n",
    "当模型中的参数较小的时候，上面的误差最小化的问题可以直接使用公式的方式解出来，这类的解被称为解析解。  \n",
    "在吴恩达老师的机器学习课程中对解析解做了一点介绍，但是本书中没有。 \n",
    "##### 解析解\n",
    "我们将所有的模型参数写成一个向量的形式。\n",
    "$$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$$\n",
    "这里的$\\theta_0$就是上面的$b$。\n",
    "同样的我们可以将特征也写作向量。\n",
    "$$X = \\begin{bmatrix}1 \\\\ X_1 \\\\ X_2\\end{bmatrix}$$\n",
    "这样我们就可以将我们模型的预测写作如下的形式。\n",
    "$$h_\\theta(x) = \\theta^TX = \\theta_0 + \\theta_1X_1 + \\theta_2X_2$$\n",
    "损失函数为：\n",
    "$$J(\\theta) = \\sum_{i=0}^n\\frac12[h_\\theta(x^{(i)}) - y^{(i)}]^2$$\n",
    "使用向量的形式书写如下：\n",
    "$$J(\\theta) = \\frac12(\\theta X - Y)^T(\\theta X - Y)$$\n",
    "使用最小二乘法可以得到$\\theta$的解析解：\n",
    "$$\\theta = (X^TX)^{-1}X^TY$$\n",
    "求一个矩阵的逆矩阵大概是$O(n^3)$数量级的，所以该方法不适合n（模型的参数的数量， 也就是$|X|$）较大的情况下。而且我们还要考虑一个矩阵的逆矩阵是否是存在的（大多数情况下都是存在的）。\n",
    "\n",
    "##### 数值解\n",
    "大多数机器学习的模型都是没有解析解的，就算是有算起来也非常的复杂，我们只能通过优化算法进行有限次的迭代模型参数来尽可能的降低损失函数的值，这类解叫做**数值解**。  \n",
    "在求数值解的算法中，**小批量随机梯度下降(mini-batch stochastic gradient descent)**在深度学习中广泛被使用。  \n",
    "首先我们选取一组模型参数的默认值（如随机选取），然后对模型参数进行迭代，使得每次迭代之后，损失函数的值都要尽可能的减少。每次迭代时，先随机均匀的采样一个由固定样本所组成的小批量${\\frak B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），然后用此结果与预先设定的一个正数(学习率 learning rate)乘积作为模型参数在本次迭代中的减少量。  \n",
    "$$\n",
    "w_1 \\leftarrow w_1 - \\frac\\eta{\\frak |B|}\\sum_{i\\in{\\frak B}}\\frac{\\partial{\\scr l}^{(i)}(w_1,\\ w_2,\\ b)}{\\partial w_1} = w_1 - \\frac\\eta{\\frak |B|}\\sum_{i\\in{\\frak B}}x_1^{(i)}(x_1^{(i)}w_1 + x_2^{(i)}w_2 + b - y^{(i)})\n",
    "$$\n",
    "$$\n",
    "w_2 \\leftarrow w_2 - \\frac\\eta{\\frak |B|}\\sum_{i\\in{\\frak B}}\\frac{\\partial{\\scr l}^{(i)}(w_1,\\ w_2,\\ b)}{\\partial w_2} = w_2 - \\frac\\eta{\\frak |B|}\\sum_{i\\in{\\frak B}}x_2^{(i)}(x_1^{(i)}w_1 + x_2^{(i)}w_2 + b - y^{(i)})\n",
    "$$\n",
    "$$\n",
    "b \\leftarrow b - \\frac\\eta{\\frak |B|}\\sum_{i\\in{\\frak B}}\\frac{\\partial{\\scr l}^{(i)}(w_1,\\ w_2,\\ b)}{\\partial b} = b - \\frac\\eta{\\frak |B|}\\sum_{i\\in{\\frak B}}(x_1^{(i)}w_1 + x_2^{(i)}w_2 + b - y^{(i)})\n",
    "$$\n",
    "在上式中，$|{\\frak B}|$代表每个小批量中的样本个数(批量大小， batch size)，$\\eta$被称为学习率(learning rate)，这些参数都是认为设定的，并不是通过模型训练得到的，因此被称为**超参数**。我们常说的调参就是指调节超参数的大小。\n",
    "\n",
    "#### 模型预测\n",
    "模型训练完成后得到的模型参数为$\\hat w_1,\\ \\hat w_2,\\ \\hat b$，此时并不一定是最优解$w_1^*,\\ w_2^*,\\ b^*$，而是对最优解的一个近似。此时我们可以通过$x_1\\hat w_1 + x_2\\hat w_2 + \\hat b$对不在训练集中的数据进行预测。这里的估算被叫做**模型预测，模型推断或模型测试。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
