{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归的简洁实现\n",
    "我们可以使用`MXNet`提供的`gluon`接口简洁的实现线性回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "\n",
    "num_input = 2\n",
    "num_example = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "\n",
    "features = nd.random.normal(scale=1, shape=(num_example, num_input))\n",
    "lables = features[:, 0] * true_w[0] + features[:, 1] * true_w[1] + true_b\n",
    "lables += nd.random.normal(scale=0.01, shape=lables.shape)\n",
    "# 以上步骤和之前的是相同的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签结合\n",
    "dataset = gdata.ArrayDataset(features, lables)\n",
    "# 随机读取小批量\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True) # 第三个参数为True表示随机读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-1.8885833   0.6752996 ]\n",
      " [-0.04226572  1.0837352 ]\n",
      " [ 2.2772653  -0.41856593]\n",
      " [ 0.8647537   0.13802782]\n",
      " [ 0.34399813  1.6508877 ]\n",
      " [ 1.0571185  -1.4458728 ]\n",
      " [-0.49286923  1.4190462 ]\n",
      " [ 0.7912779   0.6942737 ]\n",
      " [ 0.69407785 -1.6696625 ]\n",
      " [ 0.10682141 -0.46369416]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[-1.8700017  0.4188479 10.188307   5.450164  -0.7260705 11.234173\n",
      " -1.609464   3.4233677 11.26492    5.9778333]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn # nn 表示 neural networks(神经网络)\n",
    "\n",
    "net = nn.Sequential() # 一个串联各层的容器\n",
    "net.add(nn.Dense(1)) # 线性回归中的全连接层Dense， 输出的个数为1\n",
    "# 我们无需指定每一层的输入的形状，后面执行的时候会自动推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01)) # 初始化方法，将权重设置为均值为1，标准差为0.01的正态分布，偏差设置为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import loss as gloss\n",
    "\n",
    "loss = gloss.L2Loss() # 平方损失，又被称为L2范数损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate' : 0.03}) # 定义优化算法和学习率\n",
    "# sgd => stochastic gradient descent 随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 0.035208\n",
      "epoch 11, loss: 0.000122\n",
      "epoch 11, loss: 0.000048\n",
      "epoch 11, loss: 0.000048\n",
      "epoch 11, loss: 0.000049\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 # 迭代的次数。\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter: # 获取一批数据\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y) # 计算损失函数\n",
    "        l.backward() # 求导\n",
    "        trainer.step(batch_size) # 梯度下降\n",
    "    l = loss(net(features), lables) # 计算模型的loss\n",
    "    print('epoch %d, loss: %f'%(batch_size + 1, l.mean().asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4], \n",
       " [[ 2.0001264 -3.4000208]]\n",
       " <NDArray 1x2 @cpu(0)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0] # 获取训练得到的模型参数，因为输出层只有一个节点，所以就是net[0]\n",
    "true_w, dense.weight.data() # 获取训练得到的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, \n",
       " [4.20103]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, dense.bias.data() # 获取训练得到的偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 如果将`l = loss(net(X), y)`替换成`l = loss(net(X), y).mean()`，我们需要将`trainer.step(batch_size)`相应地改成`trainer.step(1)`。这是为什么呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 答： 那就要先观察一下官方文档中这个函数的定义了。  \n",
    "> `step（batch_size，ignore_stale_grad = False)`  \n",
    "> `Makes one step of parameter update. Should be called after autograd.backward() and outside of record() scope.`  \n",
    "> `batch_size (int) – Batch size of data processed. Gradient will be normalized by 1/batch_size. Set this to 1 if you normalized loss manually with loss = mean(loss).`  \n",
    "看来官方文档中已经说的足够清楚了。我们`l.backward()`相当于`l.sum().backward()`,指的是一批样本中的所有的数据的损失的梯度，我们需要传递`batch_size`，函数的内部将`/batch_size`求均值以弱化更新参数带来的影响。\n",
    "如果我们先对其进行了`mean()`操作，操作完了之后变成了$\\mathbb{R}$的一个标量，此时就不必除以`batch_size`了，将这个值设置为`1`就行了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 查阅`MXNet`的文档，看看`gluon.loss`和`init`模块中提供了哪些损失函数和初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 答:  \n",
    "> - `gluon.loss`中有很多的损失函数。  \n",
    ">   - `L1Loss`\n",
    "$$L = \\sum_i|label_i - pred_i|$$\n",
    ">   - `L2Loss`\n",
    "$$L = \\frac12\\sum_i|label_i - pred_i|^2$$\n",
    ">   - `SigmoidBinaryCrossEntropyLoss`\n",
    "$$prob = \\frac1{1 + e^{-pred}}$$\n",
    "$$L = - \\sum_ilable_i*log(prob_i) + pos\\_weight + (1 - lable_i)*log(1 - prob_i)$$\n",
    ">   - `SoftmaxCrossEntropyLoss`\n",
    "$$p = softmax(pred)$$\n",
    "$$L = -\\sum_i\\log p_{i,\\ lable_i}$$\n",
    ">   - `KLDivLoss`\n",
    "$$L = \\sum_ilable_i*[\\log(lable_i) - \\log(pred_i)]$$\n",
    ">   - `LogisticLoss`\n",
    "$$L = \\sum_i\\log\\left(1 + e^{-pred_i * lable_i}\\right)$$\n",
    "> 还有非常多的损失函数，但是我基本上都看不懂，在这儿抄一遍也没啥意思，打公式还是蛮费劲的。  \n",
    ">  \n",
    "> - `init`中有很多初始化的方法。\n",
    ">    - `Normal`  *Initializes weights with random values sampled from a normal distribution with a mean of zero and standard deviation of sigma.*\n",
    ">    - `Load`    *Initializes variables by loading data from file or dict.*\n",
    ">    - `LSTMBias`    *Initialize all biases of an LSTMCell to 0.0 except for the forget gate whose bias is set to custom value.*\n",
    ">    - `MSRAPrelu`   *Initialize the weight according to a MSRA paper.*\n",
    ">    - `Mixed`    *Initialize parameters using multiple initializers.*\n",
    ">    - `One`    *Initializes weights to one.*\n",
    ">    - `Orthogonal`  *Initialize weight as orthogonal matrix.*\n",
    ">    - `Uniform`  *Initializes weights with random values uniformly sampled from a given range.*\n",
    ">    - `Xavier`   *Returns an initializer performing “Xavier” initialization for weights.*\n",
    ">    - `Zero`   *Initializes weights to zero.*\n",
    ">    - `register`    *Registers a custom initializer.*  \n",
    ">上面就是官方文档中的所有的初始化的方法。虽然大多数我都不知道是什么意思~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如何访问`dense.weight`的梯度？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 答: 通过`dense.weight.grad()`可以访问`dense.weight`的梯度。(瞎试出来的，我先看的`dense.weight.grad`，输出告诉我这是一个函数，那么就调用这个函数就完事了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.06060244 -0.01666654]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.weight.grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
