{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise Softmax Regression\n",
    "Use gluon to implement softmax regression is also concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import d2lzh as d2l\n",
    "import utils\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define And Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10)) # 10 output(catagroies) in output layer\n",
    "# Use normal distribution with mean difference of 0 and standard deviation of 0.01 to initialize weight\n",
    "net.initialize(init.Normal(sigma=0.01)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax And Corss Entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate' : 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7879, train acc 0.748, test acc 0.805\n",
      "epoch 2, loss 0.5742, train acc 0.811, test acc 0.824\n",
      "epoch 3, loss 0.5291, train acc 0.824, test acc 0.831\n",
      "epoch 4, loss 0.5054, train acc 0.830, test acc 0.836\n",
      "epoch 5, loss 0.4895, train acc 0.834, test acc 0.840\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epoch,\n",
    "              batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try adjusting hyperparameters, such as batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer: I try 10, 100, 256, 500, it seems that the distinction is slight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
