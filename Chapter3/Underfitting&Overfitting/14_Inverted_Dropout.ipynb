{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Dropout\n",
    "Like `Weight Delay`, `Inverted Dropout` is a efficient method to slove the problem of `Underfitting and Overfitting` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "Given a multilayer perceptron with one hidden layer, the number of input unit is $5$ and the number of hidden unit. Among them, $h_i$ is i-th hidden unit.\n",
    "$$h_i = \\phi(x_1w_{1i} + x_2w_{2i} + x_3w_{3i} + x_4w_{4i} + b_i)$$\n",
    "When using dropout in this hidden layer, the hidden unit will be **dropped out with a certain probability $p$**. If not dropped out, it will be **stretched divided by $1- p$**.  \n",
    "Assume $\\xi_i$ is $0$ or $1$ with the probability $p$ and $1-p$. Using dropout method, we can get that  \n",
    "$$h_i' = \\frac{\\xi_i}{1-p}h_i$$\n",
    "If $\\xi$ is $0$, then $h_i' = 0$.  \n",
    "If $\\xi$ is $1$, then $h_i' = \\frac{h_i}{1-p}$.\n",
    "Due to $E(\\xi_i) = 1 - p$, we can get  \n",
    "$$E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i$$\n",
    "We will not change the maths expectation using the dropout method. The dropout method functions as **Regularization**. It is a efficient method to **handle the overfitting** when training datasets, but usually we don't use dropout when testing datasets to get more accurate result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import utils\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob): # 'prob' is probability\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # Drop all elements in this condition\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob # nice code here. \n",
    "    # some element will be zero here, and others will be one, which is determined by keep_prob\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, -1))\n",
    "dropout(X, 0) # None elements will be dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  4.  6.  0.  0.  0. 14.]\n",
       " [ 0. 18.  0.  0. 24. 26. 28.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5) # About 8 elements will be dropped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1) # All elements will be dropped out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Parameters\n",
    "Take Fashion-MNIST as example. Define a multilayer perceptron with two hidden layers, and there are 256 unit in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_hidden1, num_hidden2, num_outputs = 784, 256, 256, 10\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hidden1))\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hidden1, num_hidden2))\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hidden2, num_outputs))\n",
    "\n",
    "b1 = nd.zeros(num_hidden1)\n",
    "b2 = nd.zeros(num_hidden2)\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Use dropout method only when we are in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1) + b1).relu()\n",
    "    if autograd.is_training(): # if in training mode\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    H2 = (nd.dot(H1, W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1107, train acc 0.563, test acc 0.798\n",
      "epoch 2, loss 0.5754, train acc 0.787, test acc 0.841\n",
      "epoch 3, loss 0.4906, train acc 0.821, test acc 0.848\n",
      "epoch 4, loss 0.4473, train acc 0.836, test acc 0.854\n",
      "epoch 5, loss 0.4184, train acc 0.848, test acc 0.864\n"
     ]
    }
   ],
   "source": [
    "num_epoch, lr, batch_size = 5, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = utils.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epoch, batch_size, params, lr)\n",
    "\n",
    "# test accuracy is higher than train accuracy, great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Consicely by Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1966, train acc 0.535, test acc 0.793\n",
      "epoch 2, loss 0.5885, train acc 0.781, test acc 0.826\n",
      "epoch 3, loss 0.4989, train acc 0.819, test acc 0.843\n",
      "epoch 4, loss 0.4565, train acc 0.834, test acc 0.853\n",
      "epoch 5, loss 0.4219, train acc 0.846, test acc 0.861\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob1), # add dropout layer after the first fully connection layer \n",
    "        nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob2), # add dropout layer after the second fully connection layer\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate' : lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epoch, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. What if we reverse the position of probability hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add number of epoch, observe the distinction of using dropout or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Complex the model or add more hidden layer unit, will the effect of dropout more obvious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare dropout to weight delay. If simultaneously using dropout and weight delay, what will happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
