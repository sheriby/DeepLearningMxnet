{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation, Back Propagation and Computational Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "`Forward Propagation` means that calculate variable **from input layer to output layer** in neural network. Simply put, assume the input featrue is $x \\in \\mathbb R^d$ with out bias, then the intermediate variable as follows.\n",
    "$$z = W^{(1)}x$$\n",
    "Among which, $W \\in R{h \\times d}$ is the weight parameters, and intermediate variable $z \\in \\mathbb R^h$. Put $z$ into the activation function $\\phi$, we can get that.\n",
    "$$h = \\phi(z)$$\n",
    "$h$ is a intermediate variable as well, therefore we can know\n",
    "$$o = W^{(2)}h$$\n",
    "Assume the loss function is ${\\scr l}$, the loss item is \n",
    "$$L = {\\scr l}(o, y)$$\n",
    "According to the definition of L2 norm regularization, given hyperparameter $\\lambda$, the regularization item is\n",
    "$$s = \\frac\\lambda2(\\parallel\\!W^{(1)}\\!\\parallel_F^2 + \\parallel\\!W^{(2)}\\!\\parallel_F^2)$$\n",
    "Therefore, given data sample, the loss with regularization is \n",
    "$$J = L + s$$\n",
    "Among which $J$ is **Objective Function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph of Forward Propagation\n",
    "Here is a computational graph of forward propagation.  \n",
    "**Input variables** are $a$ and $b$. **Output variables** is $e$. **Intermediate variables** are $c$ and $d$.\n",
    "![](https://mlln.cn/2018/07/02/tensorflow%E6%95%99%E7%A8%8B02-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%8A%E5%85%B6%E5%AE%9E%E8%B7%B5/images/computational-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "`Back Propagation` is the method to **calculate the gradient of parameters** in neural network. Generally speaking, back propagation follows the chain rule, from the output layer to input layer.  \n",
    "For input or output $X, Y, Z$ is tensor of arbitrary shape function $Y = f(X)$ and $Z = g(y)$. According to the chain of rule, we can get  \n",
    "$$\\frac{\\partial z}{\\partial x} = prod\\left(\\frac{\\partial z}{\\partial y}, \\frac{\\partial y}{\\partial x}\\right)$$\n",
    "Among which $prod$ is matrix multiplication by some necessary procedure such as reverse and transpose.  \n",
    "For objective function $J = L + s$.\n",
    "$$\\frac{\\partial J}{\\partial L} = 1,\\quad \\frac{\\partial J}{\\partial s} = 1$$\n",
    "Calculate the gradient of output layer.\n",
    "$$\\frac{\\partial J}{\\partial o} = prod\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial o}\\right) = \\frac{\\partial L}{\\partial o}$$\n",
    "Calculate the gradient of two regularization parameters.\n",
    "$$\\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)},\\quad\\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)}$$\n",
    "Now, we can get the gradient of $W_2$ which is the closest to output layer.\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}} = prod\\left(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}}\\right) + prod\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}}\\right) = \\frac{\\partial J}{\\partial o}h^{\\mathsf T} + \\lambda W^{(2)}$$\n",
    "Continue propagation along the hidden layer, we can get.\n",
    "$$\\frac{\\partial J}{\\partial h} = prod\\left(\\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial h}\\right) = W^{(2)\\mathsf T}\\frac{\\partial J}{\\partial o}$$\n",
    "The gradient of intermediate variable $z$ as follows.\n",
    "$$\\frac{\\partial J}{\\partial z} = prob\\left(\\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z}\\right) = \\frac{\\partial J}{\\partial h}\\odot\\phi'(z)$$\n",
    "Activation function $\\phi$ is operated by element, so here using $\\odot$ operation.  \n",
    "Finally, we can get the gradient of $W^{(i)}$ which is closest to input layer.\n",
    "$$\\frac{\\partial J}{\\partial W^{(i)}} = prob\\left(\\frac{\\partial J}{\\partial z},\\frac{\\partial z}{\\partial W^{(1)}}\\right) + prob\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(1)}}\\right) = \\frac{\\partial J}{\\partial z}x^{\\mathsf T} + \\lambda W^{(1)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
