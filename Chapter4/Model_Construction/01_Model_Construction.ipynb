{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction\n",
    "It Makes it flexible to construct model that using model construction method based by `Class Block`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model by Inheriting Class Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "# construct a mutlilayer perceptron\n",
    "class MLP(nn.Block): # inherit Block\n",
    "    # declare layer with model parameters.\n",
    "    # Here declare two fully connection layer\n",
    "    def __init__(self, **kwargs):\n",
    "        # use the contruction function of super class Block to do some\n",
    "        # necessary initialization. \n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu') # hidden layer\n",
    "        self.output = nn.Dense(10) # output layer\n",
    "    # define the forward pass calculation of model.\n",
    "    # That means how to calculate output result using the input x\n",
    "    def forward(self, x): # this function shall be named as 'forward'\n",
    "        return self.output(self.hidden(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We needn't write 'backward' function in above MLP because the model will automaticly generate `backward` function by using `autograd`.  \n",
    "We can get model varialbe `net` by instantiate class `MLP`.Among them, `net(X)` will call function `__call__` inherited by class `Block`, which will call function `forward` in class MLP to do froward pass calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.09543004  0.04614332 -0.00286655 -0.07790346 -0.05130241  0.02942038\n",
       "   0.08696645 -0.0190793  -0.04122177  0.05088576]\n",
       " [ 0.0769287   0.03099706  0.00856576 -0.044672   -0.06926838  0.09132431\n",
       "   0.06786592 -0.06187843 -0.03436674  0.04234696]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(2, 20))\n",
    "net = MLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `Block` is a freely createable components. It's subclass can be a layer such as `Dense`, a model such as above `MLP`, or something like a part of model else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Sequential is Inherited by Class Block\n",
    "Here we will implement `MySequential` to simply show how class `Sequential` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MySequential, self).__init__(**kwargs)\n",
    "    \n",
    "    def add(self, block):\n",
    "        # store block variable in _children which is a `OrderedDict` and will be call when we\n",
    "        # instantiate MySequential \n",
    "        # property _children and name is inherited from class Block\n",
    "        self._children[block.name] = block\n",
    "    \n",
    "    def forward(self, X): # forward pass calculate\n",
    "        # OrderedDick will make sure that we will traverse it \n",
    "        # in the order of how they are added.\n",
    "        for block in self._children.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00362229  0.00633331  0.03201145 -0.01369375  0.10336448 -0.0350802\n",
       "  -0.00032165 -0.01676024  0.06978628  0.01303309]\n",
       " [ 0.03871717  0.02608212  0.03544958 -0.02521311  0.11005436 -0.01430663\n",
       "  -0.03052467 -0.03852826  0.06321152  0.0038594 ]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructe Complex Model\n",
    "Here we will implement a complex model `FancyMLP` inherited by class `Block`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        # rand_weight parameter in get_constant function will not be \n",
    "        # changed. It is constant.\n",
    "        self.rand_weight = self.params.get_constant(\n",
    "            'rand_weight', nd.random.uniform(shape=(20, 20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.dense(X)\n",
    "        # use constant rand_weight parameter\n",
    "        X = nd.relu(nd.dot(X, self.rand_weight.data()) + 1)\n",
    "        # reuse fully connectional layer\n",
    "        X = self.dense(X)\n",
    "        # control flow\n",
    "        while X.norm().asscalar() > 1:\n",
    "            X /= 2\n",
    "        if X.norm().asscalar() < 0.8:\n",
    "            X *= 10\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[26.700357]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'),\n",
    "                    nn.Dense(32, activation='relu'))\n",
    "        self.dense = nn.Dense(16, activation='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dense(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[27.52695]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(NestMLP(), nn.Dense(20), FancyMLP())\n",
    "\n",
    "net.initialize()\n",
    "net(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
