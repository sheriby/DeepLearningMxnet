{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n",
    "The accession, initialization and sharing of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init, nd\n",
    "from mxnet.gluon import nn \n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize() # use default initialization method.\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[0.5488135  0.5928446  0.71518934 0.84426576 0.60276335 0.8579456\n",
       "   0.5448832  0.8472517  0.4236548  0.6235637  0.6458941  0.3843817\n",
       "   0.4375872  0.2975346  0.891773   0.05671298 0.96366274 0.2726563\n",
       "   0.3834415  0.47766513]\n",
       "  [0.79172504 0.8121687  0.5288949  0.47997716 0.56804454 0.3927848\n",
       "   0.92559665 0.83607876 0.07103606 0.33739617 0.08712929 0.6481719\n",
       "   0.0202184  0.36824155 0.83261985 0.95715517 0.77815676 0.14035077\n",
       "   0.87001216 0.87008727]]\n",
       " <NDArray 2x20 @cpu(0)>, \n",
       " [[ 0.09543004  0.04614332 -0.00286655 -0.07790346 -0.05130241  0.02942038\n",
       "    0.08696645 -0.0190793  -0.04122177  0.05088576]\n",
       "  [ 0.0769287   0.03099706  0.00856576 -0.044672   -0.06926838  0.09132431\n",
       "    0.06786592 -0.06187843 -0.03436674  0.04234696]]\n",
       " <NDArray 2x10 @cpu(0)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense0_ (\n",
       "   Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "   Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       " ), mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params, type(net[0].params) # net[0] is the first added layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter dense0_weight (shape=(256, 20), dtype=float32),\n",
       " Parameter dense0_bias (shape=(256,), dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight, net[0].bias # use properties weight and bias to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[ 0.06700657 -0.00369488  0.0418822  ... -0.05517294 -0.01194733\n",
       "   -0.00369594]\n",
       "  [-0.03296221 -0.04391347  0.03839272 ...  0.05636378  0.02545484\n",
       "   -0.007007  ]\n",
       "  [-0.0196689   0.01582889 -0.00881553 ...  0.01509629 -0.01908049\n",
       "   -0.02449339]\n",
       "  ...\n",
       "  [ 0.00010955  0.0439323  -0.04911506 ...  0.06975312  0.0449558\n",
       "   -0.03283203]\n",
       "  [ 0.04106557  0.05671307 -0.00066976 ...  0.06387014 -0.01292654\n",
       "    0.00974177]\n",
       "  [ 0.00297424 -0.0281784  -0.06881659 ... -0.04047417  0.00457048\n",
       "    0.05696651]]\n",
       " <NDArray 256x20 @cpu(0)>, \n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " <NDArray 256 @cpu(0)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data(), net[0].bias.data() # use function data to get initialization ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " ...\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad()\n",
    "# using function 'grad' to get the gradient ndarray of parameters.\n",
    "# Here without backward pass calculation, all elements in gradient ndarray are still zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 10 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].bias.data() # the second added layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense0_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense1_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()\n",
    "# Using function 'collect_params', we can get all parameters in this multilayer percetorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential0_ (\n",
       "  Parameter dense0_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense1_weight (shape=(10, 256), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params('.*weight') # get all weight parameters\n",
    "# It's easy to see that all weight parameters is end with 'weight' generally,\n",
    "# such as dense0_weight and dense1_weight.\n",
    "# Using regex could match them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Share Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
       " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
       "<NDArray 8x8 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "       shared,\n",
    "       nn.Dense(8, activation='relu', params=shared.params),\n",
    "       nn.Dense(10))\n",
    "\n",
    "net.initialize()\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "Y = net(X)\n",
    "\n",
    "net[1].weight.data() == net[2].weight.data()\n",
    "# The second layer and the third layer share same weight parameters.\n",
    "# And there's no doubt that the bias is same as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
